{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqiyCijO244tkaVenElOD6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijahnzeli1/Backend-hotel/blob/main/CodeT5%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly. Let's use an alternative approach using the Hugging Face Transformers library with a pre-trained CodeT5+ model, which is specifically designed for code generation tasks. This approach doesn't require an API key and can be run locally or on your own infrastructure.\n",
        "Here's an improved version using CodeT5+:"
      ],
      "metadata": {
        "id": "k5hmhEIhel24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wTWUKRJRSS45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWkQTXfhekMd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Load CodeT5+ model and tokenizer\n",
        "model_name = \"Salesforce/codet5p-770m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Check if CUDA is available and move model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_project(project_description, programming_language, max_length=1024):\n",
        "    \"\"\"Generate a complete project, attempting to split by functional blocks.\"\"\"\n",
        "    try:\n",
        "        prompt = f\"Generate a complete project structure and code in {programming_language} for: {project_description}\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1,\n",
        "                                     temperature=0.7, top_p=0.95, do_sample=True)\n",
        "\n",
        "        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Basic functional splitting (can be improved)\n",
        "        if programming_language.lower() == \"python\":\n",
        "            code_chunks = re.split(r'(?<=def\\s+\\w+\\(.*\\):\\n)', generated_code)\n",
        "            code_chunks = [c for c in code_chunks if c.strip()]  # Remove empty chunks\n",
        "        else:\n",
        "            # Add logic for other languages based on their function/class definitions\n",
        "            code_chunks = [generated_code]  # No splitting for now\n",
        "\n",
        "        return code_chunks\n",
        "    except Exception as e:\n",
        "        return [f\"An error occurred: {str(e)}\"]  # Return as a list for consistency\n",
        "\n",
        "def review_code(project_dir, programming_language):\n",
        "    \"\"\"Perform code review using appropriate linters for different languages.\"\"\"\n",
        "    if programming_language.lower() == \"python\":\n",
        "        review_python(project_dir)\n",
        "    elif programming_language.lower() == \"java\":\n",
        "        review_java(project_dir)\n",
        "    elif programming_language.lower() == \"javascript\":\n",
        "        review_javascript(project_dir)\n",
        "    else:\n",
        "        print(f\"Code review not yet supported for {programming_language}.\")\n",
        "\n",
        "def review_python(project_dir):\n",
        "    try:\n",
        "        import pylint\n",
        "    except ImportError:\n",
        "        print(\"Installing pylint...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pylint\"])\n",
        "        import pylint\n",
        "\n",
        "    for filename in os.listdir(project_dir):\n",
        "        if filename.endswith(\".py\"):\n",
        "            filepath = os.path.join(project_dir, filename)\n",
        "            print(f\"\\nPylint review for {filename}:\")\n",
        "            subprocess.run([\"pylint\", filepath], check=False)\n",
        "\n",
        "def review_java(project_dir):\n",
        "    checkstyle_jar = \"checkstyle-8.44-all.jar\"\n",
        "    checkstyle_url = f\"https://github.com/checkstyle/checkstyle/releases/download/checkstyle-8.44/{checkstyle_jar}\"\n",
        "\n",
        "    if not os.path.exists(checkstyle_jar):\n",
        "        print(f\"Downloading {checkstyle_jar}...\")\n",
        "        subprocess.run([\"curl\", \"-L\", checkstyle_url, \"-o\", checkstyle_jar], check=True)\n",
        "\n",
        "    for filename in os.listdir(project_dir):\n",
        "        if filename.endswith(\".java\"):\n",
        "            filepath = os.path.join(project_dir, filename)\n",
        "            print(f\"\\nCheckstyle review for {filename}:\")\n",
        "            subprocess.run([\"java\", \"-jar\", checkstyle_jar, \"-c\", \"/google_checks.xml\", filepath], check=False)\n",
        "\n",
        "def review_javascript(project_dir):\n",
        "    try:\n",
        "        subprocess.run([\"eslint\", \"--version\"], check=True, capture_output=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Installing ESLint...\")\n",
        "        subprocess.run([\"npm\", \"install\", \"-g\", \"eslint\"], check=True)\n",
        "\n",
        "    for filename in os.listdir(project_dir):\n",
        "        if filename.endswith(\".js\"):\n",
        "            filepath = os.path.join(project_dir, filename)\n",
        "            print(f\"\\nESLint review for {filename}:\")\n",
        "            subprocess.run([\"eslint\", filepath], check=False)\n",
        "\n",
        "def save_project(project_name, code_chunks, programming_language):\n",
        "    \"\"\"Save the generated project, creating multiple files and directory structure.\"\"\"\n",
        "    project_dir = project_name.replace(' ', '_').lower()\n",
        "    os.makedirs(project_dir, exist_ok=True)  # Create project directory\n",
        "\n",
        "    for i, chunk in enumerate(code_chunks):\n",
        "        # Determine file extension\n",
        "        if programming_language.lower() == \"python\":\n",
        "            extension = \".py\"\n",
        "        elif programming_language.lower() == \"java\":\n",
        "            extension = \".java\"\n",
        "        elif programming_language.lower() == \"c++\":\n",
        "            extension = \".cpp\"\n",
        "        elif programming_language.lower() == \"javascript\":\n",
        "            extension = \".js\"\n",
        "        else:\n",
        "            extension = \".txt\"\n",
        "\n",
        "        filename = f\"part_{i+1}{extension}\"\n",
        "        filepath = os.path.join(project_dir, filename)\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write(chunk)\n",
        "        print(f\"Saved {filename} to {project_dir}\")\n",
        "        print(f\"Project saved to {project_dir}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the AI Project Generator!\")\n",
        "    while True:\n",
        "        project_description = input(\"Please describe your project (or type 'quit' to exit): \")\n",
        "        if project_description.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        project_name = input(\"Enter a name for your project: \")\n",
        "        programming_language = input(\"Enter your preferred programming language (e.g., Python, Java, C++): \")\n",
        "\n",
        "        print(\"Generating project... This may take a moment.\")\n",
        "        generated_code = generate_project(project_description, programming_language)\n",
        "\n",
        "        print(\"\\nGenerated Project Structure and Code:\")\n",
        "        print(generated_code)\n",
        "\n",
        "        save_project(project_name, generated_code, programming_language)\n",
        "\n",
        "        print(\"\\nWould you like to generate another project?\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "id": "5i_ouMAcSNm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly. Let's use an alternative approach using the Hugging Face Transformers library with a pre-trained CodeT5+ model, which is specifically designed for code generation tasks. This approach doesn't require an API key and can be run locally or on your own infrastructure.\n",
        "Here's an improved version using CodeT5+:\n",
        "pythonCopyimport torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "\n",
        "# Load CodeT5+ model and tokenizer\n",
        "model_name = \"Salesforce/codet5p-770m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Check if CUDA is available and move model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_project(project_description, max_length=1024):\n",
        "    \"\"\"Generate a complete project based on the user's description.\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(f\"Generate a complete project structure and code for: {project_description}\",\n",
        "                           return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1,\n",
        "                                     temperature=0.7, top_p=0.95, do_sample=True)\n",
        "\n",
        "        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_code\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "def save_project(project_name, content):\n",
        "    \"\"\"Save the generated project to a file.\"\"\"\n",
        "    filename = f\"{project_name.replace(' ', '_').lower()}.py\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(content)\n",
        "    print(f\"Project saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the AI Project Generator!\")\n",
        "    while True:\n",
        "        project_description = input(\"Please describe your project (or type 'quit' to exit): \")\n",
        "        if project_description.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        project_name = input(\"Enter a name for your project: \")\n",
        "        \n",
        "        print(\"Generating project... This may take a moment.\")\n",
        "        generated_project = generate_project(project_description)\n",
        "        \n",
        "        print(\"\\nGenerated Project Structure and Code:\")\n",
        "        print(generated_code)\n",
        "        \n",
        "        save_project(project_name, generated_code)\n",
        "        \n",
        "        print(\"\\nWould you like to generate another project?\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "To use this script:\n",
        "\n",
        "Install the required packages:\n",
        "Copypip install torch transformers\n",
        "\n",
        "Run the script and follow the prompts to describe your project.\n",
        "\n",
        "This approach offers several advantages:\n",
        "\n",
        "No API Key Required: It uses a locally downloaded model, so you don't need to manage API keys or worry about usage limits.\n",
        "CodeT5+ Model: This model is specifically trained for code generation tasks and can handle various programming languages.\n",
        "Customizable: You can easily switch to different CodeT5+ model sizes or even other code-generation models available on Hugging Face.\n",
        "GPU Acceleration: The script automatically uses GPU if available, which can significantly speed up generation for larger projects.\n",
        "Interactive Interface: Provides a user-friendly command-line interface for inputting project descriptions and names.\n",
        "File Saving: Generated projects are automatically saved to files for easy access and further editing.\n",
        "\n",
        "Keep in mind that while CodeT5+ is powerful, it may not generate complete, multi-file projects as effectively as GPT-4. You might need to provide more detailed prompts or generate code for different parts of your project separately.\n",
        "Also, remember that generated code should always be reviewed and tested, as it may contain errors or may not fully meet your specific requirements.\n",
        "To further improve this for handling larger projects:\n",
        "\n",
        "Implement a function to split larger projects into manageable chunks.\n",
        "Add support for generating multiple files and a proper directory structure.\n",
        "Incorporate a code review step using a linter or another AI model to catch potential issues.\n",
        "\n",
        "These enhancements would make the tool more robust for generating complete, production-ready projects."
      ],
      "metadata": {
        "id": "s26QtZljerNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wtprn4fxewX2"
      }
    }
  ]
}